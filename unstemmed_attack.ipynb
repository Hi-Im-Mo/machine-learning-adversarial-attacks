{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "pazYQbmUSCcv",
      "metadata": {
        "id": "pazYQbmUSCcv"
      },
      "source": [
        "# Setting up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "44126771",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44126771",
        "outputId": "5167351f-3d3d-47a4-9d98-161a213616a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: textattack in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: bert-score>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.3.13)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.10/dist-packages (from textattack) (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.13.1)\n",
            "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.10/dist-packages (from textattack) (2.7.1)\n",
            "Requirement already satisfied: lemminflect in /usr/local/lib/python3.10/dist-packages (from textattack) (0.2.3)\n",
            "Requirement already satisfied: lru-dict in /usr/local/lib/python3.10/dist-packages (from textattack) (1.3.0)\n",
            "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.17.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.11.4)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.38.1)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.10/dist-packages (from textattack) (3.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.2)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (from textattack) (1.1)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (from textattack) (0.5.13)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.1.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n",
            "Requirement already satisfied: pinyin>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.4.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n",
            "Requirement already satisfied: OpenHowNet in /usr/local/lib/python3.10/dist-packages (from textattack) (2.0)\n",
            "Requirement already satisfied: pycld2 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.41)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (23.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2023.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.4.2)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.34.53)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.3.4)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.14)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (6.1.3)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.7.3)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.2)\n",
            "Requirement already satisfied: janome>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.5.0)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.4)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.5.10)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (3.1)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.2)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.3.3)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.26.18)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.6.0)\n",
            "Requirement already satisfied: semver<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (3.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.3.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words->textattack) (0.6.2)\n",
            "Requirement already satisfied: anytree in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (2.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.53 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (1.34.53)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (0.10.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair->textattack) (0.1.99)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (4.12.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair->textattack) (6.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.3.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (3.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.27.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers>=4.30.0->textattack) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.10/dist-packages (4.6.2)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "# Google Collab installation requirements\n",
        "!pip install datasets\n",
        "!pip install textattack\n",
        "!pip install scikit-learn\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "612fe0dd",
      "metadata": {
        "id": "612fe0dd"
      },
      "outputs": [],
      "source": [
        "# Dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import textattack\n",
        "from textattack.attack_recipes import TextFoolerJin2019\n",
        "from textattack import Attacker\n",
        "from abc import ABC, abstractmethod\n",
        "from pymongo import MongoClient\n",
        "#from passwords import password"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "252fb4ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "252fb4ff",
        "outputId": "541eca89-dfce-43cb-998a-f6a3ada91171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# the Natural Language Toolkit & tokenizer\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "310977c2",
      "metadata": {
        "id": "310977c2"
      },
      "outputs": [],
      "source": [
        "# Connect to MongoDB\n",
        "\n",
        "# Initialize authentication & cursor variables\n",
        "password = 'WJwxIjCdmlGoCrN7'\n",
        "mongo_uri = f'mongodb+srv://kkosek:{password}@cluster0.lv4rmyj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'\n",
        "db_name = 'walmart'\n",
        "collection_name = 'scraper'\n",
        "\n",
        "\n",
        "# Create a client\n",
        "client = MongoClient(mongo_uri)\n",
        "# Connect to the 'walmart' database\n",
        "db = client[db_name]\n",
        "# Open the 'scraper' collection\n",
        "collection = db[collection_name]\n",
        "\n",
        "# Fetch all documents from the collection\n",
        "cursor = collection.find({})\n",
        "\n",
        "# Convert documents to a list of dictionaries\n",
        "documents = list(cursor)\n",
        "\n",
        "# Close the cursor and client\n",
        "cursor.close()\n",
        "client.close()\n",
        "\n",
        "# Convert the list of dicts to master DataFrame\n",
        "df = pd.DataFrame(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0TqfcfO5SHyT",
      "metadata": {
        "id": "0TqfcfO5SHyT"
      },
      "source": [
        "# Binary Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "836988f2",
      "metadata": {
        "id": "836988f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "4897ff5e-c44c-4577-c8ca-2d57bf3ea142"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'stars'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'stars'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-315c5beff763>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbinary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a copy of the 'stars' column where 'stars' is equal to 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mbinary_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Add a new column in the length of the DataFrame with all 1s to bin 5stars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'stars'"
          ]
        }
      ],
      "source": [
        "# Binary Sentiment Labelling\n",
        "\n",
        "# Create copy of master df\n",
        "binary_df = df.copy()\n",
        "# Create a copy of the 'stars' column where 'stars' is equal to 5\n",
        "five = binary_df.loc[ binary_df['stars'] == 5 ].copy()\n",
        "# Add a new column in the length of the DataFrame with all 1s to bin 5stars\n",
        "five['label'] = pd.Series( [x/x for x in range(1,len(five)+1)] , index=five.index )\n",
        "# Create a copy of the 'stars' column where 'stars' is equal to 1\n",
        "one = binary_df.loc[ binary_df['stars'] == 1 ].copy()\n",
        "# Add a new column in the length of the DataFrame with all 0s to bin 1stars\n",
        "one['label'] = pd.Series( [((x/x)-1) for x in range(1,len(one)+1)] , index=one.index )\n",
        "# Concat the binary sentiment df\n",
        "pos_neg = pd.concat( [five,one] )\n",
        "\n",
        "# Overwrite the master set to only contain the 'text' and 'label' data (& reset index)\n",
        "df = pos_neg[['text','label']].reset_index(drop=True)\n",
        "# Clear all non-alphabetic characters out of text file\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", str(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "frb65D3qSilR",
      "metadata": {
        "id": "frb65D3qSilR"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb9a3cb",
      "metadata": {
        "id": "1eb9a3cb"
      },
      "outputs": [],
      "source": [
        "# Separate out into features (X) and target (y)\n",
        "df_X = df['text']\n",
        "df_y = df['label']\n",
        "\n",
        "# Perform a test split on the features & target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_X, df_y, test_size=0.15)\n",
        "\n",
        "# Restore the split data into DataFrame objects now organized by training & testing\n",
        "df_train = pd.DataFrame([X_train, y_train]).T\n",
        "df_test =pd.DataFrame([X_test, y_test]).T\n",
        "\n",
        "# Reset indices\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "# Relabel our columns\n",
        "df_train.columns = ['text', 'label']\n",
        "df_test.columns = ['text', 'label']\n",
        "\n",
        "# Retype the label to be int\n",
        "df_train['label'] = df_train['label'].astype(int)\n",
        "df_test['label'] = df_test['label'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dr6dpuNwSmSM",
      "metadata": {
        "id": "Dr6dpuNwSmSM"
      },
      "source": [
        "# Fitting & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d614f4",
      "metadata": {
        "id": "b2d614f4"
      },
      "outputs": [],
      "source": [
        "# TF-IDF Vectorization of text examples\n",
        "\n",
        "# Initialize Vectorizer object\n",
        "Tfidf = TfidfVectorizer( ngram_range=(1, 3), max_features=100 )\n",
        "\n",
        "# Fit the vectorizer with the text data\n",
        "unstemmed_tfidf_vect_fit = Tfidf.fit(df_train['text'])\n",
        "\n",
        "# Using the text data, vectorize the training 'words' with respect to their own frequency\n",
        "# throughout the training corpus\n",
        "Tfidf_training = unstemmed_tfidf_vect_fit.transform(df_train['text'])\n",
        "# Convert those vectors into a DataFrame object\n",
        "df_train_tfidf_unstem = pd.DataFrame( Tfidf_training.toarray() )\n",
        "\n",
        "# Vectorize the testing 'words' with respect to their own frequency\n",
        "# throughout the training corpus\n",
        "Tfidf_testing = unstemmed_tfidf_vect_fit.transform(df_test['text'])\n",
        "# Convert those vectors into a DataFrame object\n",
        "df_test_tfidf_unstem = pd.DataFrame( Tfidf_testing.toarray() )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FJGYrTYFTTr_",
      "metadata": {
        "id": "FJGYrTYFTTr_"
      },
      "source": [
        "# Classifier & Testing Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "825db01a",
      "metadata": {
        "id": "825db01a"
      },
      "outputs": [],
      "source": [
        "# Define our classifier model\n",
        "log_reg = LogisticRegression(C=30, max_iter=200)\n",
        "# Fit that model on the stemmed & tokenized text examples with their recorded label (pos or neg)\n",
        "log_reg = log_reg.fit(df_train_tfidf_unstem, df_train[\"label\"])\n",
        "# Use fitted classifier to predict the label from the testing stems\n",
        "y_pred = log_reg.predict(df_test_tfidf_unstem)\n",
        "\n",
        "print(classification_report(df_test[\"label\"], y_pred))\n",
        "print(f\"Confusion Matrix:\\n{metrics.confusion_matrix(y_test, y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eCsID534RLMb",
      "metadata": {
        "id": "eCsID534RLMb"
      },
      "source": [
        "# Class Restructuring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bfcb930",
      "metadata": {
        "id": "5bfcb930"
      },
      "outputs": [],
      "source": [
        "# This cell bypasses a bug encountered with the source code of the API. The method '.get_feature_names()' was\n",
        "# deprecated to '.get_feature_names_out()' in scikit >= 1.0.0, so the API encounters the error during normal functioning.\n",
        "# (I don't know enough about subclassing to do it with less code than hardwiring it in a jupyter cell..)\n",
        "\n",
        "class ModelWrapper(ABC):\n",
        "    \"\"\"A model wrapper queries a model with a list of text inputs.\n",
        "\n",
        "    Classification-based models return a list of lists, where each sublist\n",
        "    represents the model's scores for a given input.\n",
        "\n",
        "    Text-to-text models return a list of strings, where each string is the\n",
        "    output – like a translation or summarization – for a given input.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, text_input_list, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_grad(self, text_input):\n",
        "        \"\"\"Get gradient of loss with respect to input tokens.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _tokenize(self, inputs):\n",
        "        \"\"\"Helper method for `tokenize`\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def tokenize(self, inputs, strip_prefix=False):\n",
        "        \"\"\"Helper method that tokenizes input strings\n",
        "        Args:\n",
        "            inputs (list[str]): list of input strings\n",
        "            strip_prefix (bool): If `True`, we strip auxiliary characters added to tokens as prefixes (e.g. \"##\" for BERT, \"Ġ\" for RoBERTa)\n",
        "        Returns:\n",
        "            tokens (list[list[str]]): List of list of tokens as strings\n",
        "        \"\"\"\n",
        "        tokens = self._tokenize(inputs)\n",
        "        if strip_prefix:\n",
        "            # `aux_chars` are known auxiliary characters that are added to tokens\n",
        "            strip_chars = [\"##\", \"Ġ\", \"__\"]\n",
        "            # TODO: Find a better way to identify prefixes. These depend on the model, so cannot be resolved in ModelWrapper.\n",
        "\n",
        "            def strip(s, chars):\n",
        "                for c in chars:\n",
        "                    s = s.replace(c, \"\")\n",
        "                return s\n",
        "\n",
        "            tokens = [[strip(t, strip_chars) for t in x] for x in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "class SklearnModelWrapper(ModelWrapper):\n",
        "    \"\"\"Loads a scikit-learn model and tokenizer (tokenizer implements\n",
        "    `transform` and model implements `predict_proba`).\n",
        "\n",
        "    May need to be extended and modified for different types of\n",
        "    tokenizers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, text_input_list, batch_size=None):\n",
        "        encoded_text_matrix = self.tokenizer.transform(text_input_list).toarray()\n",
        "        tokenized_text_df = pd.DataFrame(\n",
        "            encoded_text_matrix)\n",
        "        return self.model.predict_proba(tokenized_text_df)\n",
        "\n",
        "    def get_grad(self, text_input):\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RaD_icpIRP2z",
      "metadata": {
        "id": "RaD_icpIRP2z"
      },
      "source": [
        "# Launching our attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f67d150e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f67d150e",
        "outputId": "22d6123d-9279-4a6b-89fb-b12268c3d062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
            "100%|██████████| 481M/481M [00:34<00:00, 13.9MB/s]\n",
            "textattack: Unzipping file /root/.cache/textattack/tmpgtugauzr.zip to /root/.cache/textattack/word_embeddings/paragramcf.\n",
            "textattack: Successfully saved word_embeddings/paragramcf to cache.\n",
            "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  delete\n",
            "  )\n",
            "  (goal_function):  UntargetedClassification\n",
            "  (transformation):  WordSwapEmbedding(\n",
            "    (max_candidates):  50\n",
            "    (embedding):  WordEmbedding\n",
            "  )\n",
            "  (constraints): \n",
            "    (0): WordEmbeddingDistance(\n",
            "        (embedding):  WordEmbedding\n",
            "        (min_cos_sim):  0.5\n",
            "        (cased):  False\n",
            "        (include_unknown_words):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (1): PartOfSpeech(\n",
            "        (tagger_type):  nltk\n",
            "        (tagset):  universal\n",
            "        (allow_verb_noun_swap):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (2): UniversalSentenceEncoder(\n",
            "        (metric):  angular\n",
            "        (threshold):  0.840845057\n",
            "        (window_size):  15\n",
            "        (skip_text_shorter_than_window):  True\n",
            "        (compare_against_original):  False\n",
            "      )\n",
            "    (3): RepeatModification\n",
            "    (4): StopwordModification\n",
            "    (5): InputColumnModification(\n",
            "        (matching_column_labels):  ['premise', 'hypothesis']\n",
            "        (columns_to_ignore):  {'premise'}\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 1 / 1:  10%|█         | 1/10 [00:00<00:01,  7.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 1 ---------------------------------------------\n",
            "[[1 (60%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "BUYERS I bought an Xbox and revived a box of Valentine Little Debbie cakes \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Succeeded / Failed / Skipped / Total] 0 / 0 / 1 / 1:  20%|██        | 2/10 [00:49<03:16, 24.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 2 ---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 1 / 1 / 1 / 3:  30%|███       | 3/10 [00:49<01:55, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 (100%)]] --> [[0 (73%)]]\n",
            "\n",
            "[[Great]] [[tv]] with beautiful [[picture]] and [[good]] sound  [[Would]] definitely recommend \n",
            "\n",
            "[[Awesome]] [[broadcasting]] with beautiful [[archives]] and [[adequate]] sound  [[Do]] definitely recommend \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 3 ---------------------------------------------\n",
            "[[0 (100%)]] --> [[[FAILED]]]\n",
            "\n",
            "handle fell off within two weeks\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 2 / 1 / 1 / 4:  40%|████      | 4/10 [00:50<01:15, 12.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 4 ---------------------------------------------\n",
            "[[1 (97%)]] --> [[0 (56%)]]\n",
            "\n",
            "I read all the reviews before purchasing this mattress and felt fairly confident of the quality  I was a little uneasy buying a mattress on line but everything that I read ended up being spot on and it s a [[great]] mattress for the [[price]]  My daughter loves it and I got a [[great]] value  I highly recommend this product \n",
            "\n",
            "I read all the reviews before purchasing this mattress and felt fairly confident of the quality  I was a little uneasy buying a mattress on line but everything that I read ended up being spot on and it s a [[admirable]] mattress for the [[priced]]  My daughter loves it and I got a [[admirable]] value  I highly recommend this product \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 3 / 2 / 1 / 6:  60%|██████    | 6/10 [00:51<00:34,  8.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 5 ---------------------------------------------\n",
            "[[0 (94%)]] --> [[[FAILED]]]\n",
            "\n",
            "not adequate for even an occasional guest bed horrible  poor choice stay away\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 6 ---------------------------------------------\n",
            "[[0 (52%)]] --> [[1 (56%)]]\n",
            "\n",
            "I m upset that I can t use my tumbler because wrong straw sent with it  It wouldn t be so bad if I [[had]] an extra taller straw at home to use  Other than that the cup is beautiful \n",
            "\n",
            "I m upset that I can t use my tumbler because wrong straw sent with it  It wouldn t be so bad if I [[was]] an extra taller straw at home to use  Other than that the cup is beautiful \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 4 / 2 / 1 / 7:  70%|███████   | 7/10 [00:51<00:21,  7.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 7 ---------------------------------------------\n",
            "[[1 (66%)]] --> [[0 (56%)]]\n",
            "\n",
            "With  K resolution  it really stands out if you want to make a statement in any room  One of the best features for me was that they included Bluetooth connectivity to any headphones  so if you wanna blast that movie at night by all means  It also syncs with your PS  and your fire stick TV [[so]] you don t need so one remote \n",
            "\n",
            "With  K resolution  it really stands out if you want to make a statement in any room  One of the best features for me was that they included Bluetooth connectivity to any headphones  so if you wanna blast that movie at night by all means  It also syncs with your PS  and your fire stick TV [[after]] you don t need so one remote \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 5 / 3 / 1 / 9:  90%|█████████ | 9/10 [00:54<00:06,  6.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 8 ---------------------------------------------\n",
            "[[0 (85%)]] --> [[[FAILED]]]\n",
            "\n",
            "I bought the xbox series x online  When it got here i hooked it up and it turned on then   seconds later it turned off  I tried for a half an hour to get it to say on  Finally i got it to work and setup everything  Then start downloading the first game then it shutoff again  Did that for awhile till i got fed up and called customer service to send me a replacement \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 9 ---------------------------------------------\n",
            "[[1 (100%)]] --> [[0 (55%)]]\n",
            "\n",
            "[[Great]] [[price]] for what you get\n",
            "\n",
            "[[Awesome]] [[airfare]] for what you get\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 6 / 3 / 1 / 10: 100%|██████████| 10/10 [00:54<00:00,  5.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 10 ---------------------------------------------\n",
            "[[1 (94%)]] --> [[0 (77%)]]\n",
            "\n",
            "I had been wanting a larger screen TV than a    inch  So I went to Walmart to check out the larger screen TV  I ended up getting a Vizio    inch   k uhd led  I [[love]] it has a fantastic vivid picture  Most of a [[great]] low Walmart price  Very happy \n",
            "\n",
            "I had been wanting a larger screen TV than a    inch  So I went to Walmart to check out the larger screen TV  I ended up getting a Vizio    inch   k uhd led  I [[adore]] it has a fantastic vivid picture  Most of a [[awesome]] low Walmart price  Very happy \n",
            "\n",
            "\n",
            "\n",
            "+-------------------------------+--------+\n",
            "| Attack Results                |        |\n",
            "+-------------------------------+--------+\n",
            "| Number of successful attacks: | 6      |\n",
            "| Number of failed attacks:     | 3      |\n",
            "| Number of skipped attacks:    | 1      |\n",
            "| Original accuracy:            | 90.0%  |\n",
            "| Accuracy under attack:        | 30.0%  |\n",
            "| Attack success rate:          | 66.67% |\n",
            "| Average perturbed word %:     | 15.31% |\n",
            "| Average num. words per input: | 33.9   |\n",
            "| Avg num queries:              | 215.67 |\n",
            "+-------------------------------+--------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7d42c8cff520>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7d42cba6c340>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7d42d07fba00>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7d42cbc17100>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7d42d03ef6a0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7d42d14c1db0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7d42d03ef0a0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7d42c86330d0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7d42c8630070>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7d42c8632e30>]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Define a wrapper object to function similarly to a pipeline and hold our fitted classifier model\n",
        "# with our fitted vectorizor. The wrapper is engineered to function within the textattack architecture\n",
        "model_wrapper = SklearnModelWrapper(log_reg, unstemmed_tfidf_vect_fit)\n",
        "\n",
        "# The textattack architecture functions on textattack.datasets.Dataset objects\n",
        "# The convertor accepts a list of tuples containing inputs and output examples\n",
        "# For instance, (\"I like this product\", 1) represents a tuple containing an input and output\n",
        "# Thus, we create a list comprehension to compile the df['text'] & df['label'] into this format\n",
        "data = [(df_train['text'][x], int(df_train['label'][x])) for x in range(0,(len(df_train)))]\\\n",
        "# Then we call the textattack converter to assemble our data into the architecture\n",
        "dataset = textattack.datasets.Dataset(data)\n",
        "\n",
        "# The attack recipe in this attack is based on TextFooler, an adversarial attacker on NLP datasets that functions\n",
        "# by trying out iterations of tokens that can be modified slightly to cause the model to misclassify the sentiment\n",
        "# This attacker is pretrained and loaded into textattack libraries, so we can call it and build the adversarial\n",
        "# model based on our NLP model\n",
        "attack = TextFoolerJin2019.build(model_wrapper)\n",
        "# We can specifically add more arguments to the Attacker class, but the pretrained model is optimal as is\n",
        "attacker = Attacker(attack, dataset)\n",
        "# Call .attack_dataset() to create .attack instances across the whole dataset\n",
        "attacker.attack_dataset()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "frb65D3qSilR",
        "Dr6dpuNwSmSM",
        "FJGYrTYFTTr_",
        "eCsID534RLMb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bootcampDev",
      "language": "python",
      "name": "bootcampdev"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}