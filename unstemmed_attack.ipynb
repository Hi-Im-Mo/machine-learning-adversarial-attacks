{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "pazYQbmUSCcv",
      "metadata": {
        "id": "pazYQbmUSCcv"
      },
      "source": [
        "# Setting up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "44126771",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44126771",
        "outputId": "0cb88afe-5325-4b9e-d153-656dbca4e5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n",
            "Collecting textattack\n",
            "  Downloading textattack-0.3.9-py3-none-any.whl (436 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bert-score>=0.3.5 (from textattack)\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n",
            "Collecting flair (from textattack)\n",
            "  Downloading flair-0.13.1-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.13.1)\n",
            "Collecting language-tool-python (from textattack)\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Collecting lemminflect (from textattack)\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lru-dict (from textattack)\n",
            "  Downloading lru_dict-1.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.18.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.11.4)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.38.1)\n",
            "Collecting terminaltables (from textattack)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.2)\n",
            "Collecting word2number (from textattack)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting num2words (from textattack)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.1.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n",
            "Collecting pinyin>=0.4.0 (from textattack)\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n",
            "Collecting OpenHowNet (from textattack)\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Collecting pycld2 (from textattack)\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting click<8.1.0 (from textattack)\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (23.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2023.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.4.2)\n",
            "Collecting boto3>=1.20.27 (from flair->textattack)\n",
            "  Downloading boto3-1.34.54-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2 (from flair->textattack)\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting conllu>=4.0 (from flair->textattack)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair->textattack)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair->textattack)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.7.3)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.2)\n",
            "Collecting janome>=0.4.2 (from flair->textattack)\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect>=1.0.9 (from flair->textattack)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.4)\n",
            "Collecting mpld3>=0.3 (from flair->textattack)\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pptree>=3.1 (from flair->textattack)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair->textattack)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.2)\n",
            "Collecting segtok>=1.5.11 (from flair->textattack)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair->textattack)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair->textattack)\n",
            "  Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl (14 kB)\n",
            "Collecting urllib3<2.0.0,>=1.0.0 (from flair->textattack)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia-api>=0.5.7 (from flair->textattack)\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting semver<4.0.0,>=3.0.0 (from flair->textattack)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.3.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->textattack)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting anytree (from OpenHowNet->textattack)\n",
            "  Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n",
            "Collecting botocore<1.35.0,>=1.34.54 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading botocore-1.34.54-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair->textattack) (0.1.99)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (4.12.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair->textattack) (6.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.3.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (3.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
            "Collecting accelerate>=0.21.0 (from transformers>=4.30.0->textattack)\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers>=4.30.0->textattack) (5.9.5)\n",
            "Building wheels for collected packages: pinyin, pycld2, word2number, docopt, langdetect, pptree, sqlitedict\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630476 sha256=326e1d81c36c0d035cfe2baf3840ac33d068ea89b288df6fec5ffe5c844a29a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp310-cp310-linux_x86_64.whl size=9904024 sha256=d13367bb7b83b76e9194665b8063a0e63e7e27088532bd9634ca43b2070a42f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/81/31/240c89c845e008a93d98542325270007de595bfd356eb0b06c\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=aac258214da29f956b885f412285d48d311974b3e49157f553fa5c874b2f4aa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=da81641abcd9c587153ea37929f837b2dba89256907a82275627d9f07b16d4da\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=1a6803156d125278fada3626f633d74e84c6bb419c9c61736ae03b9e752a926d\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=00042a8e3f2feaaa35fcc5e16e93ff912346e7edd2574193547a7f027407c4aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=d8cbb007728dcaf650e2fb6764de21c8ea982998fa7f7b8b614efb1f20fa7351\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built pinyin pycld2 word2number docopt langdetect pptree sqlitedict\n",
            "Installing collected packages: word2number, sqlitedict, pycld2, pptree, pinyin, janome, docopt, urllib3, terminaltables, semver, segtok, num2words, lru-dict, lemminflect, langdetect, jmespath, ftfy, deprecated, conllu, click, anytree, botocore, wikipedia-api, s3transfer, pytorch-revgrad, OpenHowNet, mpld3, language-tool-python, bpemb, boto3, accelerate, bert-score, transformer-smaller-training-vocab, flair, textattack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "Successfully installed OpenHowNet-2.0 accelerate-0.27.2 anytree-2.12.1 bert-score-0.3.13 boto3-1.34.54 botocore-1.34.54 bpemb-0.3.4 click-8.0.4 conllu-4.5.3 deprecated-1.2.14 docopt-0.6.2 flair-0.13.1 ftfy-6.1.3 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.3.0 mpld3-0.5.10 num2words-0.5.13 pinyin-0.4.0 pptree-3.1 pycld2-0.41 pytorch-revgrad-0.2.0 s3transfer-0.10.0 segtok-1.5.11 semver-3.0.2 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.9 transformer-smaller-training-vocab-0.3.3 urllib3-1.26.18 wikipedia-api-0.6.0 word2number-1.1\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.2/677.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.6.1 pymongo-4.6.2\n"
          ]
        }
      ],
      "source": [
        "# Google Collab installation requirements\n",
        "!pip install datasets\n",
        "!pip install textattack\n",
        "!pip install scikit-learn\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "612fe0dd",
      "metadata": {
        "id": "612fe0dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208a3a1e-f6a3-48e3-dad4-f651827d374f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Updating TextAttack package dependencies.\n",
            "textattack: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import textattack\n",
        "from textattack.attack_recipes import TextFoolerJin2019\n",
        "from textattack import Attacker\n",
        "from abc import ABC, abstractmethod\n",
        "from pymongo import MongoClient\n",
        "#from passwords import password\n",
        "password = 'WJwxIjCdmlGoCrN7'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "252fb4ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "252fb4ff",
        "outputId": "1e62bb79-9512-4525-e6a7-1f43d2633838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# the Natural Language Toolkit & tokenizer\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "310977c2",
      "metadata": {
        "id": "310977c2"
      },
      "outputs": [],
      "source": [
        "# Connect to MongoDB\n",
        "\n",
        "# Initialize authentication & cursor variables\n",
        "mongo_uri = f'mongodb+srv://kkosek:{password}@cluster0.lv4rmyj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'\n",
        "db_name = 'walmart'\n",
        "collection_name = 'scraper'\n",
        "\n",
        "\n",
        "# Create a client\n",
        "client = MongoClient(mongo_uri)\n",
        "# Connect to the 'walmart' database\n",
        "db = client[db_name]\n",
        "# Open the 'scraper' collection\n",
        "collection = db[collection_name]\n",
        "\n",
        "# Fetch all documents from the collection\n",
        "cursor = collection.find({})\n",
        "\n",
        "# Convert documents to a list of dictionaries\n",
        "documents = list(cursor)\n",
        "\n",
        "# Close the cursor and client\n",
        "cursor.close()\n",
        "client.close()\n",
        "\n",
        "# Convert the list of dicts to master DataFrame\n",
        "df = pd.DataFrame(documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ts4W1oUeGvfe",
        "outputId": "59f8ede0-72c4-4636-a675-738d00f0db6d"
      },
      "id": "ts4W1oUeGvfe",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           _id        date  \\\n",
              "0     65e0f530f115d6fa0351af26    2/7/2024   \n",
              "1     65e0f530f115d6fa0351af27   11/1/2023   \n",
              "2     65e0f530f115d6fa0351af28  11/22/2023   \n",
              "3     65e0f530f115d6fa0351af29   12/5/2023   \n",
              "4     65e0f530f115d6fa0351af2a  12/24/2023   \n",
              "...                        ...         ...   \n",
              "7789  65e0f530f115d6fa0351cd93    3/8/2023   \n",
              "7790  65e0f530f115d6fa0351cd94   3/12/2023   \n",
              "7791  65e0f530f115d6fa0351cd95    4/2/2023   \n",
              "7792  65e0f530f115d6fa0351cd96  11/30/2021   \n",
              "7793  65e0f530f115d6fa0351cd97   12/8/2019   \n",
              "\n",
              "                                                   text  stars  \n",
              "0     Unfortunately the tv fell on me after purchasi...      1  \n",
              "1     I love Vizio that’s always my go to brand I ha...      1  \n",
              "2     I ordered a Vizio 50\" Class V-Series 4L UHD LE...      1  \n",
              "3     It didn't work right out of the box. Nothing b...      1  \n",
              "4     I ordered  75” Vizio TV and just put it up, bu...      1  \n",
              "...                                                 ...    ...  \n",
              "7789  Might want order 1 size larger because of lini...      4  \n",
              "7790  Love these crocs. Great for winter time. Also,...      4  \n",
              "7791  Comfy! Perfect for work! Just wish the inside ...      4  \n",
              "7792  The seem to fit tighter than my normal crocs. ...      4  \n",
              "7793  I love these shoes, this is my third pair. I h...      4  \n",
              "\n",
              "[7794 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38c5a1e6-6e3b-4e2a-8103-62d9626ee5be\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>65e0f530f115d6fa0351af26</td>\n",
              "      <td>2/7/2024</td>\n",
              "      <td>Unfortunately the tv fell on me after purchasi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65e0f530f115d6fa0351af27</td>\n",
              "      <td>11/1/2023</td>\n",
              "      <td>I love Vizio that’s always my go to brand I ha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>65e0f530f115d6fa0351af28</td>\n",
              "      <td>11/22/2023</td>\n",
              "      <td>I ordered a Vizio 50\" Class V-Series 4L UHD LE...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>65e0f530f115d6fa0351af29</td>\n",
              "      <td>12/5/2023</td>\n",
              "      <td>It didn't work right out of the box. Nothing b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>65e0f530f115d6fa0351af2a</td>\n",
              "      <td>12/24/2023</td>\n",
              "      <td>I ordered  75” Vizio TV and just put it up, bu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7789</th>\n",
              "      <td>65e0f530f115d6fa0351cd93</td>\n",
              "      <td>3/8/2023</td>\n",
              "      <td>Might want order 1 size larger because of lini...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7790</th>\n",
              "      <td>65e0f530f115d6fa0351cd94</td>\n",
              "      <td>3/12/2023</td>\n",
              "      <td>Love these crocs. Great for winter time. Also,...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7791</th>\n",
              "      <td>65e0f530f115d6fa0351cd95</td>\n",
              "      <td>4/2/2023</td>\n",
              "      <td>Comfy! Perfect for work! Just wish the inside ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7792</th>\n",
              "      <td>65e0f530f115d6fa0351cd96</td>\n",
              "      <td>11/30/2021</td>\n",
              "      <td>The seem to fit tighter than my normal crocs. ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7793</th>\n",
              "      <td>65e0f530f115d6fa0351cd97</td>\n",
              "      <td>12/8/2019</td>\n",
              "      <td>I love these shoes, this is my third pair. I h...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7794 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38c5a1e6-6e3b-4e2a-8103-62d9626ee5be')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-38c5a1e6-6e3b-4e2a-8103-62d9626ee5be button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-38c5a1e6-6e3b-4e2a-8103-62d9626ee5be');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cee7aae2-b5cd-4be2-b2b6-bfeb32b57dc0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cee7aae2-b5cd-4be2-b2b6-bfeb32b57dc0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cee7aae2-b5cd-4be2-b2b6-bfeb32b57dc0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3e5abffa-fdf8-4bde-beef-0920b2bcf454\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3e5abffa-fdf8-4bde-beef-0920b2bcf454 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "repr_error": "'str' object has no attribute 'empty'"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0TqfcfO5SHyT",
      "metadata": {
        "id": "0TqfcfO5SHyT"
      },
      "source": [
        "# Binary Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "836988f2",
      "metadata": {
        "id": "836988f2"
      },
      "outputs": [],
      "source": [
        "# Binary Sentiment Labelling\n",
        "\n",
        "# Create copy of master df\n",
        "binary_df = df.copy()\n",
        "# Create a copy of the 'stars' column where 'stars' is equal to 5\n",
        "five = binary_df.loc[ binary_df['stars'] == 5 ].copy()\n",
        "# Add a new column in the length of the DataFrame with all 1s to bin 5stars\n",
        "five['label'] = pd.Series( [x/x for x in range(1,len(five)+1)] , index=five.index )\n",
        "# Create a copy of the 'stars' column where 'stars' is equal to 1\n",
        "one = binary_df.loc[ binary_df['stars'] == 1 ].copy()\n",
        "# Add a new column in the length of the DataFrame with all 0s to bin 1stars\n",
        "one['label'] = pd.Series( [((x/x)-1) for x in range(1,len(one)+1)] , index=one.index )\n",
        "# Concat the binary sentiment df\n",
        "pos_neg = pd.concat( [five,one] )\n",
        "\n",
        "# Overwrite the master set to only contain the 'text' and 'label' data (& reset index)\n",
        "df = pos_neg[['text','label']].reset_index(drop=True)\n",
        "# Clear all non-alphabetic characters out of text file\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", str(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "frb65D3qSilR",
      "metadata": {
        "id": "frb65D3qSilR"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1eb9a3cb",
      "metadata": {
        "id": "1eb9a3cb"
      },
      "outputs": [],
      "source": [
        "# Separate out into features (X) and target (y)\n",
        "df_X = df['text']\n",
        "df_y = df['label']\n",
        "\n",
        "# Perform a test split on the features & target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_X, df_y, test_size=0.15)\n",
        "\n",
        "# Restore the split data into DataFrame objects now organized by training & testing\n",
        "df_train = pd.DataFrame([X_train, y_train]).T\n",
        "df_test =pd.DataFrame([X_test, y_test]).T\n",
        "\n",
        "# Reset indices\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "# Relabel our columns\n",
        "df_train.columns = ['text', 'label']\n",
        "df_test.columns = ['text', 'label']\n",
        "\n",
        "# Retype the label to be int\n",
        "df_train['label'] = df_train['label'].astype(int)\n",
        "df_test['label'] = df_test['label'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dr6dpuNwSmSM",
      "metadata": {
        "id": "Dr6dpuNwSmSM"
      },
      "source": [
        "# Fitting & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b2d614f4",
      "metadata": {
        "id": "b2d614f4"
      },
      "outputs": [],
      "source": [
        "# TF-IDF Vectorization of text examples\n",
        "\n",
        "# Initialize Vectorizer object\n",
        "Tfidf = TfidfVectorizer( ngram_range=(1, 3), max_features=100 )\n",
        "\n",
        "# Fit the vectorizer with the text data\n",
        "unstemmed_tfidf_vect_fit = Tfidf.fit(df_train['text'])\n",
        "\n",
        "# Using the text data, vectorize the training 'words' with respect to their own frequency\n",
        "# throughout the training corpus\n",
        "Tfidf_training = unstemmed_tfidf_vect_fit.transform(df_train['text'])\n",
        "# Convert those vectors into a DataFrame object\n",
        "df_train_tfidf_unstem = pd.DataFrame( Tfidf_training.toarray() )\n",
        "\n",
        "# Vectorize the testing 'words' with respect to their own frequency\n",
        "# throughout the training corpus\n",
        "Tfidf_testing = unstemmed_tfidf_vect_fit.transform(df_test['text'])\n",
        "# Convert those vectors into a DataFrame object\n",
        "df_test_tfidf_unstem = pd.DataFrame( Tfidf_testing.toarray() )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FJGYrTYFTTr_",
      "metadata": {
        "id": "FJGYrTYFTTr_"
      },
      "source": [
        "# Classifier & Testing Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "825db01a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "825db01a",
        "outputId": "8a8e7044-1841-4ef5-8fc9-83fef8a8dc72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.85      0.81       247\n",
            "           1       0.87      0.80      0.83       311\n",
            "\n",
            "    accuracy                           0.82       558\n",
            "   macro avg       0.82      0.82      0.82       558\n",
            "weighted avg       0.82      0.82      0.82       558\n",
            "\n",
            "Confusion Matrix:\n",
            "[[209  38]\n",
            " [ 63 248]]\n"
          ]
        }
      ],
      "source": [
        "# Define our classifier model\n",
        "log_reg = LogisticRegression(C=30, max_iter=200)\n",
        "# Fit that model on the stemmed & tokenized text examples with their recorded label (pos or neg)\n",
        "log_reg = log_reg.fit(df_train_tfidf_unstem, df_train[\"label\"])\n",
        "# Use fitted classifier to predict the label from the testing stems\n",
        "y_pred = log_reg.predict(df_test_tfidf_unstem)\n",
        "\n",
        "print(classification_report(df_test[\"label\"], y_pred))\n",
        "print(f\"Confusion Matrix:\\n{metrics.confusion_matrix(y_test, y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eCsID534RLMb",
      "metadata": {
        "id": "eCsID534RLMb"
      },
      "source": [
        "# Class Restructuring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5bfcb930",
      "metadata": {
        "id": "5bfcb930"
      },
      "outputs": [],
      "source": [
        "# This cell bypasses a bug encountered with the source code of the API. The method '.get_feature_names()' was\n",
        "# deprecated to '.get_feature_names_out()' in scikit >= 1.0.0, so the API encounters the error during normal functioning.\n",
        "# (I don't know enough about subclassing to do it with less code than hardwiring it in a jupyter cell..)\n",
        "\n",
        "class ModelWrapper(ABC):\n",
        "    \"\"\"A model wrapper queries a model with a list of text inputs.\n",
        "\n",
        "    Classification-based models return a list of lists, where each sublist\n",
        "    represents the model's scores for a given input.\n",
        "\n",
        "    Text-to-text models return a list of strings, where each string is the\n",
        "    output – like a translation or summarization – for a given input.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, text_input_list, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_grad(self, text_input):\n",
        "        \"\"\"Get gradient of loss with respect to input tokens.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _tokenize(self, inputs):\n",
        "        \"\"\"Helper method for `tokenize`\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def tokenize(self, inputs, strip_prefix=False):\n",
        "        \"\"\"Helper method that tokenizes input strings\n",
        "        Args:\n",
        "            inputs (list[str]): list of input strings\n",
        "            strip_prefix (bool): If `True`, we strip auxiliary characters added to tokens as prefixes (e.g. \"##\" for BERT, \"Ġ\" for RoBERTa)\n",
        "        Returns:\n",
        "            tokens (list[list[str]]): List of list of tokens as strings\n",
        "        \"\"\"\n",
        "        tokens = self._tokenize(inputs)\n",
        "        if strip_prefix:\n",
        "            # `aux_chars` are known auxiliary characters that are added to tokens\n",
        "            strip_chars = [\"##\", \"Ġ\", \"__\"]\n",
        "            # TODO: Find a better way to identify prefixes. These depend on the model, so cannot be resolved in ModelWrapper.\n",
        "\n",
        "            def strip(s, chars):\n",
        "                for c in chars:\n",
        "                    s = s.replace(c, \"\")\n",
        "                return s\n",
        "\n",
        "            tokens = [[strip(t, strip_chars) for t in x] for x in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "class SklearnModelWrapper(ModelWrapper):\n",
        "    \"\"\"Loads a scikit-learn model and tokenizer (tokenizer implements\n",
        "    `transform` and model implements `predict_proba`).\n",
        "\n",
        "    May need to be extended and modified for different types of\n",
        "    tokenizers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, text_input_list, batch_size=None):\n",
        "        encoded_text_matrix = self.tokenizer.transform(text_input_list).toarray()\n",
        "        tokenized_text_df = pd.DataFrame(\n",
        "\n",
        "            # Remove depracated \"get_feature_names() method\"\n",
        "\n",
        "            encoded_text_matrix)\n",
        "        return self.model.predict_proba(tokenized_text_df)\n",
        "\n",
        "    def get_grad(self, text_input):\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RaD_icpIRP2z",
      "metadata": {
        "id": "RaD_icpIRP2z"
      },
      "source": [
        "# Launching our attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f67d150e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f67d150e",
        "outputId": "f7c5a2b7-8687-486c-a479-5261d8991f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
            "100%|██████████| 481M/481M [00:22<00:00, 21.2MB/s]\n",
            "textattack: Unzipping file /root/.cache/textattack/tmpj2clm90f.zip to /root/.cache/textattack/word_embeddings/paragramcf.\n",
            "textattack: Successfully saved word_embeddings/paragramcf to cache.\n",
            "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  delete\n",
            "  )\n",
            "  (goal_function):  UntargetedClassification\n",
            "  (transformation):  WordSwapEmbedding(\n",
            "    (max_candidates):  50\n",
            "    (embedding):  WordEmbedding\n",
            "  )\n",
            "  (constraints): \n",
            "    (0): WordEmbeddingDistance(\n",
            "        (embedding):  WordEmbedding\n",
            "        (min_cos_sim):  0.5\n",
            "        (cased):  False\n",
            "        (include_unknown_words):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (1): PartOfSpeech(\n",
            "        (tagger_type):  nltk\n",
            "        (tagset):  universal\n",
            "        (allow_verb_noun_swap):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (2): UniversalSentenceEncoder(\n",
            "        (metric):  angular\n",
            "        (threshold):  0.840845057\n",
            "        (window_size):  15\n",
            "        (skip_text_shorter_than_window):  True\n",
            "        (compare_against_original):  False\n",
            "      )\n",
            "    (3): RepeatModification\n",
            "    (4): StopwordModification\n",
            "    (5): InputColumnModification(\n",
            "        (matching_column_labels):  ['premise', 'hypothesis']\n",
            "        (columns_to_ignore):  {'premise'}\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:22<03:22, 22.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 1 ---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|█         | 1/10 [00:23<03:27, 23.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 (79%)]] --> [[0 (58%)]]\n",
            "\n",
            "so [[good]]  the sound could be a little better  is not different than the     Vizio  but everything else is [[good]] and the image is clear \n",
            "\n",
            "so [[adequate]]  the sound could be a little better  is not different than the     Vizio  but everything else is [[decent]] and the image is clear \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|██        | 2/10 [00:23<01:33, 11.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 2 ---------------------------------------------\n",
            "[[0 (77%)]] --> [[1 (50%)]]\n",
            "\n",
            "[[When]] I [[got]] it via pick up it was cracked and I didn t notice until I [[got]] home and [[got]] it out of the bag to wash it \n",
            "\n",
            "[[Once]] I [[is]] it via pick up it was cracked and I didn t notice until I [[took]] home and [[am]] it out of the bag to wash it \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:  30%|███       | 3/10 [00:23<00:55,  7.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 3 ---------------------------------------------\n",
            "[[0 (71%)]] --> [[1 (51%)]]\n",
            "\n",
            "I bought the xbox series x online  [[When]] it [[got]] here i hooked it up and it turned on then   seconds later it turned off  I tried for a half an hour to [[get]] it to say on  Finally i [[got]] it to work and setup everything  Then start downloading the first game then it shutoff again  Did that for awhile till i [[got]] fed up and called customer service to send me a replacement \n",
            "\n",
            "I bought the xbox series x online  [[Whenever]] it [[became]] here i hooked it up and it turned on then   seconds later it turned off  I tried for a half an hour to [[achieve]] it to say on  Finally i [[am]] it to work and setup everything  Then start downloading the first game then it shutoff again  Did that for awhile till i [[am]] fed up and called customer service to send me a replacement \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 4 / 0 / 0 / 4:  40%|████      | 4/10 [00:24<00:36,  6.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 4 ---------------------------------------------\n",
            "[[1 (99%)]] --> [[0 (58%)]]\n",
            "\n",
            "I [[love]] my new tumbler     The handle is a game changer and the lid can be used with a straw or you can rotate the black part to just drink straight from the tumble  It s awesome I [[love]] the lilac color as [[well]] it s so soft \n",
            "\n",
            "I [[adore]] my new tumbler     The handle is a game changer and the lid can be used with a straw or you can rotate the black part to just drink straight from the tumble  It s awesome I [[adore]] the lilac color as [[right]] it s so soft \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 5 / 0 / 0 / 5:  50%|█████     | 5/10 [00:24<00:24,  4.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 5 ---------------------------------------------\n",
            "[[0 (77%)]] --> [[1 (51%)]]\n",
            "\n",
            "I [[do]] not like this mattress  I thought I just [[had]] to [[get]] used to it  but three months in and I m not a fan  I actually took this mattress off of my bed and put the old one back on  Id love to swap this one out for something better  This mattress is extremely hard and has   give  [[It]] s almost like [[sleeping]] on the [[floor]]  Way too firm \n",
            "\n",
            "I [[am]] not like this mattress  I thought I just [[am]] to [[achieve]] used to it  but three months in and I m not a fan  I actually took this mattress off of my bed and put the old one back on  Id love to swap this one out for something better  This mattress is extremely hard and has   give  [[He]] s almost like [[bed]] on the [[soil]]  Way too firm \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 5 / 1 / 1 / 7:  70%|███████   | 7/10 [00:25<00:10,  3.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 6 ---------------------------------------------\n",
            "[[1 (100%)]] --> [[[FAILED]]]\n",
            "\n",
            "I love my TV great price for a great picture \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 7 ---------------------------------------------\n",
            "[[1 (54%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "Aesthetically  this TV has great picture  sound and functionality  However  buyers beware  My phone was easily hacked by connecting my phone to thee tvs in my house when we had someone possibly sneaking in  and they were able to completely all my personal and work information while in my own home  spying on me  This I d also say  did have to do with security through wifi  Bluetooth and the cellular device itself  Still  the TV was a piece of the problem  Please help  \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 6 / 2 / 1 / 9:  90%|█████████ | 9/10 [00:25<00:02,  2.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 8 ---------------------------------------------\n",
            "[[0 (91%)]] --> [[[FAILED]]]\n",
            "\n",
            "I had this TV for just over a year  and then it just stopped  It wouldn t turn on or anything     I went out and bought a Samsung  and will never get a Vizio again \n",
            "\n",
            "\n",
            "--------------------------------------------- Result 9 ---------------------------------------------\n",
            "[[0 (79%)]] --> [[1 (64%)]]\n",
            "\n",
            "Unfortunately the [[TV]] is awesome but the delivery of it broke the [[screen]] on the right side  So I returned it  And went with Target purchase in stock for a   in unfortunately\n",
            "\n",
            "Unfortunately the [[TELEVISION]] is awesome but the delivery of it broke the [[screens]] on the right side  So I returned it  And went with Target purchase in stock for a   in unfortunately\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 6 / 3 / 1 / 10: 100%|██████████| 10/10 [00:26<00:00,  2.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 10 ---------------------------------------------\n",
            "[[0 (75%)]] --> [[[FAILED]]]\n",
            "\n",
            "It lacks about   inches of mattress to cover the whole space of my ikea bed  Idk why but this is my first mattress with that issue  and I m not the only one hear explaining this situation\n",
            "\n",
            "\n",
            "\n",
            "+-------------------------------+--------+\n",
            "| Attack Results                |        |\n",
            "+-------------------------------+--------+\n",
            "| Number of successful attacks: | 6      |\n",
            "| Number of failed attacks:     | 3      |\n",
            "| Number of skipped attacks:    | 1      |\n",
            "| Original accuracy:            | 90.0%  |\n",
            "| Accuracy under attack:        | 30.0%  |\n",
            "| Attack success rate:          | 66.67% |\n",
            "| Average perturbed word %:     | 8.24%  |\n",
            "| Average num. words per input: | 44.6   |\n",
            "| Avg num queries:              | 174.11 |\n",
            "+-------------------------------+--------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7fc7b5acc5b0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7fc7a993fe50>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7fc7b2022950>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7fc7b5d8dd80>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7fc7b5e33a30>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fc7b5a81360>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7fc7b20227d0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fc7abf34310>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7fc7b20228c0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7fc7a953d120>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Define a wrapper object to function similarly to a pipeline and hold our fitted classifier model\n",
        "# with our fitted vectorizor. The wrapper is engineered to function within the textattack architecture\n",
        "model_wrapper = SklearnModelWrapper(log_reg, unstemmed_tfidf_vect_fit)\n",
        "\n",
        "# The textattack architecture functions on textattack.datasets.Dataset objects\n",
        "# The convertor accepts a list of tuples containing inputs and output examples\n",
        "# For instance, (\"I like this product\", 1) represents a tuple containing an input and output\n",
        "# Thus, we create a list comprehension to compile the df['text'] & df['label'] into this format\n",
        "data = [(df_train['text'][x], int(df_train['label'][x])) for x in range(0,(len(df_train)))]\\\n",
        "# Then we call the textattack converter to assemble our data into the architecture\n",
        "dataset = textattack.datasets.Dataset(data)\n",
        "\n",
        "# The attack recipe in this attack is based on TextFooler, an adversarial attacker on NLP datasets that functions\n",
        "# by trying out iterations of tokens that can be modified slightly to cause the model to misclassify the sentiment\n",
        "# This attacker is pretrained and loaded into textattack libraries, so we can call it and build the adversarial\n",
        "# model based on our NLP model\n",
        "attack = TextFoolerJin2019.build(model_wrapper)\n",
        "# We can specifically add more arguments to the Attacker class, but the pretrained model is optimal as is\n",
        "attacker = Attacker(attack, dataset)\n",
        "# Call .attack_dataset() to create .attack instances across the whole dataset\n",
        "attacker.attack_dataset()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "frb65D3qSilR",
        "FJGYrTYFTTr_"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bootcampDev",
      "language": "python",
      "name": "bootcampdev"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}