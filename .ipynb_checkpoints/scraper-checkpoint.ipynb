{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc572e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import splinter\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bf1ad",
   "metadata": {},
   "source": [
    "# Setting up our scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eded8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Browser session\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6732a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RESTART the df when compiling new filter_ data\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3a4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an inidividual Walmart link for testing\n",
    "filter_ = 1\n",
    "# Create URL object\n",
    "website = f'https://www.walmart.com/reviews/product/389642865?filter={filter_}'\n",
    "# Visit website in our Browser session\n",
    "browser.visit(website)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad5ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important to pause briefly after the landing page loads because the bots detect rapid activity\n",
    "# and classify as spider\n",
    "\n",
    "# Scrape the landing page html\n",
    "html = browser.html\n",
    "# Use Beautiful soup to parse the webpage\n",
    "soup = BeautifulSoup(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74db986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty reviews list\n",
    "review_list = []\n",
    "# Save all of the review containers into a list object\n",
    "rows = soup.find_all(\"li\", class_=\"dib w-100 mb3\")\n",
    "\n",
    "# Create a for loop to loop through the containers and pick up the data, text & rating\n",
    "for i in rows:\n",
    "    # Try for text data\n",
    "    try:\n",
    "        # Save date text\n",
    "        date = i.find('div', class_='f7 gray mt1').text\n",
    "        # Save review text\n",
    "        text = i.find('span', class_='tl-m mb3 db-m').text\n",
    "        # Save stars text\n",
    "        stars = i.find('span', class_='w_iUH7').text\n",
    "        # Add info dict to the empty reviews_list\n",
    "        review_list.append( {\n",
    "            'date':date,\n",
    "            'text':text,\n",
    "            'stars':stars\n",
    "            } )\n",
    "    # If text is empty, print \"empty text\" to terminal\n",
    "    except AttributeError:\n",
    "        print(\"empty text\")\n",
    "\n",
    "# Transform list of dicts into a df\n",
    "review_df = pd.DataFrame(review_list)\n",
    "# Append review_df to the master df\n",
    "df = pd.concat([df,review_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f19bcfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually kill session\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f87b01a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/7/2024</td>\n",
       "      <td>Unfortunately the tv fell on me after purchasi...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/1/2023</td>\n",
       "      <td>I love Vizio that’s always my go to brand I ha...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/22/2023</td>\n",
       "      <td>I ordered a Vizio 50\" Class V-Series 4L UHD LE...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/5/2023</td>\n",
       "      <td>It didn't work right out of the box. Nothing b...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/24/2023</td>\n",
       "      <td>I ordered  75” Vizio TV and just put it up, bu...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12/2/2023</td>\n",
       "      <td>Bought the 65inch Vizio online and it arrived ...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9/7/2023</td>\n",
       "      <td>From the moment the TV arrived, albeit quickly...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8/4/2023</td>\n",
       "      <td>Ordered a 75 inch television as soon as the dr...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8/11/2023</td>\n",
       "      <td>Came broken, still waiting on refund. Took for...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10/22/2023</td>\n",
       "      <td>If you buy a TV Get it from the store and don'...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10/14/2023</td>\n",
       "      <td>The tv was broke the delivery person wasn't ab...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9/13/2023</td>\n",
       "      <td>I'm a bit disappointed that I have to wait 48 ...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10/11/2023</td>\n",
       "      <td>I've always been a huge fan of Vizio TVs for y...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9/26/2023</td>\n",
       "      <td>This is my second time within the last month a...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12/30/2023</td>\n",
       "      <td>Aside from the home not being available in my ...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12/17/2023</td>\n",
       "      <td>Order left outside in the rain propped up side...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11/11/2023</td>\n",
       "      <td>I received a used tv. Really disappointed. Scr...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12/18/2023</td>\n",
       "      <td>I bought this on Black Friday and it came with...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11/25/2023</td>\n",
       "      <td>Unpackaged tv as per the Instructions. And onc...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12/13/2023</td>\n",
       "      <td>I have never had any other brand TVs besides V...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                               text  \\\n",
       "0     2/7/2024  Unfortunately the tv fell on me after purchasi...   \n",
       "1    11/1/2023  I love Vizio that’s always my go to brand I ha...   \n",
       "2   11/22/2023  I ordered a Vizio 50\" Class V-Series 4L UHD LE...   \n",
       "3    12/5/2023  It didn't work right out of the box. Nothing b...   \n",
       "4   12/24/2023  I ordered  75” Vizio TV and just put it up, bu...   \n",
       "5    12/2/2023  Bought the 65inch Vizio online and it arrived ...   \n",
       "6     9/7/2023  From the moment the TV arrived, albeit quickly...   \n",
       "7     8/4/2023  Ordered a 75 inch television as soon as the dr...   \n",
       "8    8/11/2023  Came broken, still waiting on refund. Took for...   \n",
       "9   10/22/2023  If you buy a TV Get it from the store and don'...   \n",
       "10  10/14/2023  The tv was broke the delivery person wasn't ab...   \n",
       "11   9/13/2023  I'm a bit disappointed that I have to wait 48 ...   \n",
       "12  10/11/2023  I've always been a huge fan of Vizio TVs for y...   \n",
       "13   9/26/2023  This is my second time within the last month a...   \n",
       "14  12/30/2023  Aside from the home not being available in my ...   \n",
       "15  12/17/2023  Order left outside in the rain propped up side...   \n",
       "16  11/11/2023  I received a used tv. Really disappointed. Scr...   \n",
       "17  12/18/2023  I bought this on Black Friday and it came with...   \n",
       "18  11/25/2023  Unpackaged tv as per the Instructions. And onc...   \n",
       "19  12/13/2023  I have never had any other brand TVs besides V...   \n",
       "\n",
       "                      stars  \n",
       "0   1 out of 5 stars review  \n",
       "1   1 out of 5 stars review  \n",
       "2   1 out of 5 stars review  \n",
       "3   1 out of 5 stars review  \n",
       "4   1 out of 5 stars review  \n",
       "5   1 out of 5 stars review  \n",
       "6   1 out of 5 stars review  \n",
       "7   1 out of 5 stars review  \n",
       "8   1 out of 5 stars review  \n",
       "9   1 out of 5 stars review  \n",
       "10  1 out of 5 stars review  \n",
       "11  1 out of 5 stars review  \n",
       "12  1 out of 5 stars review  \n",
       "13  1 out of 5 stars review  \n",
       "14  1 out of 5 stars review  \n",
       "15  1 out of 5 stars review  \n",
       "16  1 out of 5 stars review  \n",
       "17  1 out of 5 stars review  \n",
       "18  1 out of 5 stars review  \n",
       "19  1 out of 5 stars review  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length check\n",
    "df = df.drop_duplicates()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5aa595",
   "metadata": {},
   "source": [
    "# Automating the scraping bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880ead29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a method to do each of the actions outlined in the last section automatically\n",
    "\n",
    "# Define scrape and take in the # of stars, # of pages to loop, and DataFrame\n",
    "def scrape(filter_, page_num_list, df):\n",
    "    # Initialize the loop through pages\n",
    "    for i in page_num_list:\n",
    "\n",
    "        # Conditional to check if this is the first run or not\n",
    "        if int(len(df)) < 1:\n",
    "            # Initialize the Browser\n",
    "            browser = Browser('chrome')\n",
    "            # Compile URL with filter_\n",
    "            website = f'https://www.walmart.com/reviews/product/1773088381?filter={filter_}'\n",
    "            # Sleep the terminal for 3 seconds to simulate human activity\n",
    "            sleep(3)\n",
    "            # Visit the website\n",
    "            browser.visit(website)        \n",
    "            # Sleep the terminal for 3 seconds to simulate human activity\n",
    "            sleep(3)\n",
    "            # Scrape the HTML of the landing page\n",
    "            html = browser.html\n",
    "            # Feed HTML to parser\n",
    "            soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "            # Clear reviews list\n",
    "            review_list = []\n",
    "            # Save all of the review containers into a list object\n",
    "            rows = soup.find_all(\"li\", class_=\"dib w-100 mb3\")\n",
    "\n",
    "            # Create a for loop to loop through the containers and pick up the data, text & rating\n",
    "            for i in rows: \n",
    "                # Try for text data\n",
    "                try:\n",
    "                    # Save date text\n",
    "                    date = i.find('div', class_='f7 gray mt1').text\n",
    "                    # Save review text\n",
    "                    text = i.find('span', class_='tl-m mb3 db-m').text\n",
    "                    # Save stars text\n",
    "                    stars = i.find('span', class_='w_iUH7').text\n",
    "                    # Add info dict to the empty reviews_list\n",
    "                    review_list.append( {\n",
    "                        'date':date,\n",
    "                        'text':text,\n",
    "                        'stars':stars\n",
    "                    } )\n",
    "                # If empty, print \"empty text\" to console\n",
    "                except AttributeError:\n",
    "                    print(\"empty text\")\n",
    "            # Transform list of dicts into a df\n",
    "            review_df = pd.DataFrame(review_list)\n",
    "            # Append review_df to the master df\n",
    "            df = pd.concat([df, review_df])\n",
    "            # Print new length of master df to console\n",
    "            print(f\"-- Length of df -- {len(df)} --\")\n",
    "            # Kill the session\n",
    "            browser.quit()\n",
    "            # Sleep the terminal for random # between 110 and 140 seconds to simulate human activity\n",
    "            sleep(int(random.uniform(110,140)))\n",
    "\n",
    "        # If this isn't the first run:\n",
    "        else:\n",
    "            # Initialize the Browser\n",
    "            browser = Browser('chrome')\n",
    "            # Compile URL with filter_\n",
    "            website = f'https://www.walmart.com/reviews/product/1773088381?filter={filter_}&page={i}'\n",
    "            # Sleep the terminal for 3 seconds to simulate human activity\n",
    "            sleep(3)\n",
    "            # Visit the website\n",
    "            browser.visit(website)\n",
    "            # Sleep the terminal for 3 seconds to simulate human activity\n",
    "            sleep(3)\n",
    "            # Scrape the HTML of the landing page\n",
    "            html = browser.html\n",
    "            soup = BeautifulSoup(html,\"html.parser\")\n",
    "            review_list = []\n",
    "            # Save all of the review containers into a list object\n",
    "            rows = soup.find_all(\"li\", class_=\"dib w-100 mb3\")\n",
    "\n",
    "            # Create a for loop to loop through the containers and pick up the data, text & rating\n",
    "            for i in rows: \n",
    "                # Try for text data\n",
    "                try:\n",
    "                    # Save date text\n",
    "                    date = i.find('div', class_='f7 gray mt1').text\n",
    "                    # Save review text\n",
    "                    text = i.find('span', class_='tl-m mb3 db-m').text\n",
    "                    # Save stars text\n",
    "                    stars = i.find('span', class_='w_iUH7').text\n",
    "                    # Add info dict to the empty reviews_list\n",
    "                    review_list.append( {\n",
    "                        'date':date,\n",
    "                        'text':text,\n",
    "                        'stars':stars\n",
    "                    } )\n",
    "                # If empty, print \"empty text\" to console\n",
    "                except AttributeError:\n",
    "                    print(\"empty text\")\n",
    "            # Transform list of dicts into a df\n",
    "            review_df = pd.DataFrame(review_list)\n",
    "            # Append review_df to the master df\n",
    "            df = pd.concat([df, review_df])\n",
    "            # Print new length of master df to console\n",
    "            print(f\"-- Length of df -- {len(df)} --\")\n",
    "            # Kill the session\n",
    "            browser.quit()\n",
    "            # Sleep the terminal for random # between 110 and 140 seconds to simulate human activity\n",
    "            sleep(int(random.uniform(110,140)))\n",
    "    # Remeber to assign this function to a variable\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d090e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Length of df -- 0 --\n",
      "-- Length of df -- 20 --\n",
      "-- Length of df -- 40 --\n",
      "-- Length of df -- 60 --\n",
      "-- Length of df -- 80 --\n",
      "-- Length of df -- 100 --\n",
      "-- Length of df -- 120 --\n",
      "-- Length of df -- 140 --\n",
      "-- Length of df -- 160 --\n",
      "-- Length of df -- 180 --\n",
      "-- Length of df -- 200 --\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "-- Length of df -- 215 --\n",
      "--- Finished compiling 1 star Data ---\n",
      "-- Length of df -- 20 --\n",
      "-- Length of df -- 40 --\n",
      "-- Length of df -- 60 --\n",
      "-- Length of df -- 80 --\n",
      "-- Length of df -- 100 --\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "-- Length of df -- 106 --\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "-- Length of df -- 106 --\n",
      "-- Length of df -- 106 --\n",
      "-- Length of df -- 106 --\n",
      "-- Length of df -- 106 --\n",
      "-- Length of df -- 106 --\n",
      "-- Length of df -- 106 --\n",
      "--- Finished compiling 2 star Data ---\n",
      "-- Length of df -- 20 --\n",
      "-- Length of df -- 40 --\n",
      "-- Length of df -- 60 --\n",
      "-- Length of df -- 80 --\n",
      "-- Length of df -- 100 --\n",
      "-- Length of df -- 120 --\n",
      "-- Length of df -- 140 --\n",
      "-- Length of df -- 160 --\n",
      "empty text\n",
      "empty text\n",
      "-- Length of df -- 178 --\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "-- Length of df -- 178 --\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "-- Length of df -- 178 --\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "empty text\n",
      "-- Length of df -- 178 --\n",
      "--- Finished compiling 3 star Data ---\n",
      "-- Length of df -- 20 --\n",
      "-- Length of df -- 40 --\n",
      "-- Length of df -- 60 --\n",
      "-- Length of df -- 80 --\n",
      "-- Length of df -- 100 --\n",
      "-- Length of df -- 120 --\n",
      "-- Length of df -- 140 --\n",
      "-- Length of df -- 160 --\n",
      "-- Length of df -- 180 --\n",
      "-- Length of df -- 200 --\n",
      "-- Length of df -- 220 --\n",
      "-- Length of df -- 240 --\n",
      "--- Finished compiling 4 star Data ---\n",
      "-- Length of df -- 20 --\n",
      "-- Length of df -- 40 --\n",
      "-- Length of df -- 60 --\n",
      "-- Length of df -- 80 --\n",
      "-- Length of df -- 100 --\n",
      "-- Length of df -- 120 --\n",
      "-- Length of df -- 140 --\n",
      "-- Length of df -- 160 --\n",
      "-- Length of df -- 180 --\n",
      "-- Length of df -- 200 --\n",
      "-- Length of df -- 220 --\n",
      "-- Length of df -- 240 --\n",
      "--- Finished compiling 5 star Data ---\n"
     ]
    }
   ],
   "source": [
    "# ONLY RESTART the df when compiling new filter_ data\n",
    "df = pd.DataFrame()\n",
    "filter_ = [1,2,3,4,5]\n",
    "\n",
    "for i in filter_:\n",
    "    df = pd.DataFrame()\n",
    "    page_num_list = list(range(13))\n",
    "    df = scrape(i,  page_num_list[1:], df)\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(f'data/cup_scraped_{i}star_data.csv', index=False)\n",
    "    print(f\"--- Finished compiling {i} star Data ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b70022e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill any remaining session\n",
    "# browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcampDev",
   "language": "python",
   "name": "bootcampdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
