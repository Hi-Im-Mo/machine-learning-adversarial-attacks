{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pazYQbmUSCcv",
   "metadata": {
    "id": "pazYQbmUSCcv"
   },
   "source": [
    "# Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "44126771",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21441,
     "status": "ok",
     "timestamp": 1709100894549,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "44126771",
    "outputId": "0a5eb504-3f5f-4ea0-f859-c5246732d57e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: textattack in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
      "Requirement already satisfied: bert-score>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.3.13)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n",
      "Requirement already satisfied: flair in /usr/local/lib/python3.10/dist-packages (from textattack) (0.13.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.13.1)\n",
      "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.10/dist-packages (from textattack) (2.7.1)\n",
      "Requirement already satisfied: lemminflect in /usr/local/lib/python3.10/dist-packages (from textattack) (0.2.3)\n",
      "Requirement already satisfied: lru-dict in /usr/local/lib/python3.10/dist-packages (from textattack) (1.3.0)\n",
      "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.17.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.25.2)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.11.4)\n",
      "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.1.0+cu121)\n",
      "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.37.2)\n",
      "Requirement already satisfied: terminaltables in /usr/local/lib/python3.10/dist-packages (from textattack) (3.1.10)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.2)\n",
      "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (from textattack) (1.1)\n",
      "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (from textattack) (0.5.13)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n",
      "Requirement already satisfied: pinyin>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.4.0)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n",
      "Requirement already satisfied: OpenHowNet in /usr/local/lib/python3.10/dist-packages (from textattack) (2.0)\n",
      "Requirement already satisfied: pycld2 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.41)\n",
      "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (2.31.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (23.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2023.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.4.2)\n",
      "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.34.51)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.3.4)\n",
      "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.5.3)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.14)\n",
      "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (6.1.3)\n",
      "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.7.3)\n",
      "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.2)\n",
      "Requirement already satisfied: janome>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.5.0)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.0.9)\n",
      "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.4)\n",
      "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.5.10)\n",
      "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (3.1)\n",
      "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.2)\n",
      "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.5.11)\n",
      "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (2.1.0)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n",
      "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.3.3)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.26.18)\n",
      "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.6.0)\n",
      "Requirement already satisfied: semver<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (3.0.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.3.2)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words->textattack) (0.6.2)\n",
      "Requirement already satisfied: anytree in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (2.12.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.51 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (1.34.51)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (0.10.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair->textattack) (0.1.99)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.14.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.3)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.13)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (4.12.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair->textattack) (6.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2024.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.3.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (3.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.27.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers>=4.30.0->textattack) (5.9.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Google Collab installation requirements\n",
    "!pip install datasets\n",
    "!pip install textattack\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "612fe0dd",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1709100894549,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "612fe0dd"
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import textattack\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack import Attacker\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "252fb4ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1709100894549,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "252fb4ff",
    "outputId": "4f5ce860-0449-4822-8a41-f3fde5040e3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the Natural Language Toolkit & tokenizer\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "310977c2",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1709100894549,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "310977c2"
   },
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "\n",
    "# Initialize authentication & cursor variables\n",
    "mongo_uri = f'mongodb+srv://kkosek:{password}@cluster0.lv4rmyj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'\n",
    "db_name = 'walmart'\n",
    "collection_name = 'scraper'\n",
    "\n",
    "# Create a client\n",
    "client = MongoClient(mongo_uri)\n",
    "# Connect to the 'walmart' database\n",
    "db = client[db_name]  \n",
    "# Open the 'scraper' collection\n",
    "collection = db[collection_name] \n",
    "\n",
    "# Fetch all documents from the collection\n",
    "cursor = collection.find({})\n",
    "\n",
    "# Convert documents to a list of dictionaries\n",
    "documents = list(cursor)\n",
    "\n",
    "# Close the cursor and client\n",
    "cursor.close()\n",
    "client.close()\n",
    "\n",
    "# Convert the list of dicts to master DataFrame\n",
    "df = pd.DataFrame(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0TqfcfO5SHyT",
   "metadata": {
    "id": "0TqfcfO5SHyT"
   },
   "source": [
    "# Binary Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "836988f2",
   "metadata": {
    "executionInfo": {
     "elapsed": 178,
     "status": "ok",
     "timestamp": 1709100894721,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "836988f2"
   },
   "outputs": [],
   "source": [
    "# Binary Sentiment Labelling\n",
    "\n",
    "# Create copy of master df\n",
    "binary_df = df.copy()\n",
    "# Create a copy of the 'stars' column where 'stars' is equal to 5\n",
    "five = binary_df.loc[ binary_df['stars'] == 5 ].copy()\n",
    "# Add a new column in the length of the DataFrame with all 1s to bin 5stars\n",
    "five['label'] = pd.Series( [x/x for x in range(1,len(five)+1)] , index=five.index )\n",
    "# Create a copy of the 'stars' column where 'stars' is equal to 1\n",
    "one = binary_df.loc[ binary_df['stars'] == 1 ].copy()\n",
    "# Add a new column in the length of the DataFrame with all 0s to bin 1stars\n",
    "one['label'] = pd.Series( [((x/x)-1) for x in range(1,len(one)+1)] , index=one.index )\n",
    "# Concat the binary sentiment df\n",
    "pos_neg = pd.concat( [five,one] )\n",
    "\n",
    "# Overwrite the master set to only contain the 'text' and 'label' data (& reset index)\n",
    "df = pos_neg[['text','label']].reset_index(drop=True)\n",
    "# Clear all non-alphabetic characters out of text file\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frb65D3qSilR",
   "metadata": {
    "id": "frb65D3qSilR"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1eb9a3cb",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1709100894721,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "1eb9a3cb"
   },
   "outputs": [],
   "source": [
    "# Separate out into features (X) and target (y)\n",
    "df_X = df['text']\n",
    "df_y = df['label']\n",
    "\n",
    "# Perform a test split on the features & target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_X, df_y, test_size=0.15)\n",
    "\n",
    "# Restore the split data into DataFrame objects now organized by training & testing\n",
    "df_train = pd.DataFrame([X_train, y_train]).T\n",
    "df_test =pd.DataFrame([X_test, y_test]).T\n",
    "\n",
    "# Reset indices\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "# Relabel our columns\n",
    "df_train.columns = ['text', 'label']\n",
    "df_test.columns = ['text', 'label']\n",
    "\n",
    "# Retype the label to be int\n",
    "df_train['label'] = df_train['label'].astype(int)\n",
    "df_test['label'] = df_test['label'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dr6dpuNwSmSM",
   "metadata": {
    "id": "Dr6dpuNwSmSM"
   },
   "source": [
    "# Fitting & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2d614f4",
   "metadata": {
    "executionInfo": {
     "elapsed": 2089,
     "status": "ok",
     "timestamp": 1709100896808,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "b2d614f4"
   },
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization of text examples\n",
    "\n",
    "# Initialize Vectorizer object\n",
    "Tfidf = TfidfVectorizer( ngram_range=(1, 3), max_features=100 )\n",
    "\n",
    "# Fit the vectorizer with the text data\n",
    "unstemmed_tfidf_vect_fit = Tfidf.fit(df_train['text'])\n",
    "\n",
    "# Using the text data, vectorize the training 'words' with respect to their own frequency\n",
    "# throughout the training corpus\n",
    "Tfidf_training = unstemmed_tfidf_vect_fit.transform(df_train['text'])\n",
    "# Convert those vectors into a DataFrame object\n",
    "df_train_tfidf_unstem = pd.DataFrame( Tfidf_training.toarray() )\n",
    "\n",
    "# Vectorize the testing 'words' with respect to their own frequency\n",
    "# throughout the training corpus\n",
    "Tfidf_testing = unstemmed_tfidf_vect_fit.transform(df_test['text'])\n",
    "# Convert those vectors into a DataFrame object\n",
    "df_test_tfidf_unstem = pd.DataFrame( Tfidf_testing.toarray() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FJGYrTYFTTr_",
   "metadata": {
    "id": "FJGYrTYFTTr_"
   },
   "source": [
    "# Classifier & Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "825db01a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1709100896809,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "825db01a",
    "outputId": "b0b9f681-165a-4ac5-bff1-e8e137f1924f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       276\n",
      "           1       0.84      0.83      0.83       282\n",
      "\n",
      "    accuracy                           0.83       558\n",
      "   macro avg       0.83      0.83      0.83       558\n",
      "weighted avg       0.83      0.83      0.83       558\n",
      "\n",
      "Confusion Matrix:\n",
      "[[230  46]\n",
      " [ 48 234]]\n"
     ]
    }
   ],
   "source": [
    "# Define our classifier model\n",
    "log_reg = LogisticRegression(C=30, max_iter=200)\n",
    "# Fit that model on the stemmed & tokenized text examples with their recorded label (pos or neg)\n",
    "log_reg = log_reg.fit(df_train_tfidf_unstem, df_train[\"label\"])\n",
    "# Use fitted classifier to predict the label from the testing stems\n",
    "y_pred = log_reg.predict(df_test_tfidf_unstem)\n",
    "\n",
    "print(classification_report(df_test[\"label\"], y_pred))\n",
    "print(f\"Confusion Matrix:\\n{metrics.confusion_matrix(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eCsID534RLMb",
   "metadata": {
    "id": "eCsID534RLMb"
   },
   "source": [
    "# Class Restructuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5bfcb930",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1709100896809,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "5bfcb930"
   },
   "outputs": [],
   "source": [
    "# This cell bypasses a bug encountered with the source code of the API. The method '.get_feature_names()' was\n",
    "# deprecated to '.get_feature_names_out()' in scikit >= 1.0.0, so the API encounters the error during normal functioning.\n",
    "# (I don't know enough about subclassing to do it with less code than hardwiring it in a jupyter cell..)\n",
    "\n",
    "class ModelWrapper(ABC):\n",
    "    \"\"\"A model wrapper queries a model with a list of text inputs.\n",
    "\n",
    "    Classification-based models return a list of lists, where each sublist\n",
    "    represents the model's scores for a given input.\n",
    "\n",
    "    Text-to-text models return a list of strings, where each string is the\n",
    "    output – like a translation or summarization – for a given input.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, text_input_list, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_grad(self, text_input):\n",
    "        \"\"\"Get gradient of loss with respect to input tokens.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _tokenize(self, inputs):\n",
    "        \"\"\"Helper method for `tokenize`\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tokenize(self, inputs, strip_prefix=False):\n",
    "        \"\"\"Helper method that tokenizes input strings\n",
    "        Args:\n",
    "            inputs (list[str]): list of input strings\n",
    "            strip_prefix (bool): If `True`, we strip auxiliary characters added to tokens as prefixes (e.g. \"##\" for BERT, \"Ġ\" for RoBERTa)\n",
    "        Returns:\n",
    "            tokens (list[list[str]]): List of list of tokens as strings\n",
    "        \"\"\"\n",
    "        tokens = self._tokenize(inputs)\n",
    "        if strip_prefix:\n",
    "            # `aux_chars` are known auxiliary characters that are added to tokens\n",
    "            strip_chars = [\"##\", \"Ġ\", \"__\"]\n",
    "            # TODO: Find a better way to identify prefixes. These depend on the model, so cannot be resolved in ModelWrapper.\n",
    "\n",
    "            def strip(s, chars):\n",
    "                for c in chars:\n",
    "                    s = s.replace(c, \"\")\n",
    "                return s\n",
    "\n",
    "            tokens = [[strip(t, strip_chars) for t in x] for x in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "class SklearnModelWrapper(ModelWrapper):\n",
    "    \"\"\"Loads a scikit-learn model and tokenizer (tokenizer implements\n",
    "    `transform` and model implements `predict_proba`).\n",
    "\n",
    "    May need to be extended and modified for different types of\n",
    "    tokenizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, text_input_list, batch_size=None):\n",
    "        encoded_text_matrix = self.tokenizer.transform(text_input_list).toarray()\n",
    "        tokenized_text_df = pd.DataFrame(\n",
    "            encoded_text_matrix)\n",
    "        return self.model.predict_proba(tokenized_text_df)\n",
    "\n",
    "    def get_grad(self, text_input):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RaD_icpIRP2z",
   "metadata": {
    "id": "RaD_icpIRP2z"
   },
   "source": [
    "# Launching our attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f67d150e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17962,
     "status": "ok",
     "timestamp": 1709100914765,
     "user": {
      "displayName": "Kicker Kosek",
      "userId": "15797899738041730348"
     },
     "user_tz": 300
    },
    "id": "f67d150e",
    "outputId": "0d4cb876-bab2-41a3-9d3b-4237d08ce635"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.840845057\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 0 / 1 / 1:  10%|█         | 1/10 [00:00<00:00, 118.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[1 (76%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "After one night sleeping on new bed my daughter s eyes  were almost swollen shut  day   her face red and swollen  day    and she is still crying in misery  What is in this mattress    I need a refund ASAP   \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 1 / 2:  20%|██        | 2/10 [00:13<00:55,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[1 (100%)]] --> [[0 (82%)]]\n",
      "\n",
      "[[Easy]] to order     pick up  Works [[great]] \n",
      "\n",
      "[[Convenience]] to order     pick up  Works [[admirable]] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 3 / 5:  50%|█████     | 5/10 [00:14<00:14,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[1 (94%)]] --> [[0 (61%)]]\n",
      "\n",
      "I [[love]] them  I [[love]] that I can just slip them on and have my feet be warm  They are so comfortable  I am going to look at another color for a variety  I m not the type of person who goes for pointed heels or tight binding shoes that hurt your feet  I love comfort and a shoe that can also be warn with just about anything  When I take them off I feel like I m still wearing comfort \n",
      "\n",
      "I [[adore]] them  I [[loves]] that I can just slip them on and have my feet be warm  They are so comfortable  I am going to look at another color for a variety  I m not the type of person who goes for pointed heels or tight binding shoes that hurt your feet  I love comfort and a shoe that can also be warn with just about anything  When I take them off I feel like I m still wearing comfort \n",
      "\n",
      "\n",
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[1 (51%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "My daughter complained everyday that her skin itches every night\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[0 (78%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "I don t know for sure if this is a tv my son will like yet since it is a Christmas gift  He will not receive it till then\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 3 / 6:  60%|██████    | 6/10 [00:15<00:10,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[1 (80%)]] --> [[0 (73%)]]\n",
      "\n",
      "It s very [[easy]] to sign in on the tv   I like it s it s very nice   Nice screen I purchase another one\n",
      "\n",
      "It s very [[simple]] to sign in on the tv   I like it s it s very nice   Nice screen I purchase another one\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 1 / 3 / 7:  70%|███████   | 7/10 [00:16<00:06,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[0 (74%)]] --> [[[FAILED]]]\n",
      "\n",
      "It came broken in this corner  I m going to return it \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Succeeded / Failed / Skipped / Total] 3 / 2 / 3 / 8:  80%|████████  | 8/10 [00:16<00:04,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[0 (96%)]] --> [[[FAILED]]]\n",
      "\n",
      "Handle broke after a few months of use\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 2 / 3 / 9:  90%|█████████ | 9/10 [00:17<00:01,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[1 (100%)]] --> [[0 (54%)]]\n",
      "\n",
      "[[I]] Bought this for my brother and he was [[very]] pleased  [[I]] was able to ship it to his address  I [[love]] that  [[Easy]] transaction\n",
      "\n",
      "[[me]] Bought this for my brother and he was [[absolutely]] pleased  [[me]] was able to ship it to his address  I [[adore]] that  [[Comfortably]] transaction\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 2 / 3 / 10: 100%|██████████| 10/10 [00:18<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[0 (88%)]] --> [[1 (52%)]]\n",
      "\n",
      "The [[screen]] was shattered when we [[bought]] it so we returned it   Usually Visio brand is our favorite \n",
      "\n",
      "The [[dropper]] was shattered when we [[purchase]] it so we returned it   Usually Visio brand is our favorite \n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 5      |\n",
      "| Number of failed attacks:     | 2      |\n",
      "| Number of skipped attacks:    | 3      |\n",
      "| Original accuracy:            | 70.0%  |\n",
      "| Accuracy under attack:        | 20.0%  |\n",
      "| Attack success rate:          | 71.43% |\n",
      "| Average perturbed word %:     | 13.26% |\n",
      "| Average num. words per input: | 25.3   |\n",
      "| Avg num queries:              | 110.86 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7bdb3c972950>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7bdb3d040a90>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7bdb3cc374f0>,\n",
       " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7bdb3ccca170>,\n",
       " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7bdb3ccca6b0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7bdb3c973bb0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7bdb3d1c15a0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7bdb3d041e40>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7bdb3d040be0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7bdb3d043670>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a wrapper object to function similarly to a pipeline and hold our fitted classifier model\n",
    "# with our fitted vectorizor. The wrapper is engineered to function within the textattack architecture\n",
    "model_wrapper = SklearnModelWrapper(log_reg, unstemmed_tfidf_vect_fit)\n",
    "\n",
    "# The textattack architecture functions on textattack.datasets.Dataset objects\n",
    "# The convertor accepts a list of tuples containing inputs and output examples\n",
    "# For instance, (\"I like this product\", 1) represents a tuple containing an input and output\n",
    "# Thus, we create a list comprehension to compile the df['text'] & df['label'] into this format\n",
    "data = [(df_train['text'][x], int(df_train['label'][x])) for x in range(0,(len(df_train)))]\\\n",
    "# Then we call the textattack converter to assemble our data into the architecture\n",
    "dataset = textattack.datasets.Dataset(data)\n",
    "\n",
    "# The attack recipe in this attack is based on TextFooler, an adversarial attacker on NLP datasets that functions\n",
    "# by trying out iterations of tokens that can be modified slightly to cause the model to misclassify the sentiment\n",
    "# This attacker is pretrained and loaded into textattack libraries, so we can call it and build the adversarial\n",
    "# model based on our NLP model\n",
    "attack = TextFoolerJin2019.build(model_wrapper)\n",
    "# We can specifically add more arguments to the Attacker class, but the pretrained model is optimal as is\n",
    "attacker = Attacker(attack, dataset)\n",
    "# Call .attack_dataset() to create .attack instances across the whole dataset\n",
    "attacker.attack_dataset()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pazYQbmUSCcv",
    "0TqfcfO5SHyT",
    "frb65D3qSilR",
    "Dr6dpuNwSmSM",
    "FJGYrTYFTTr_",
    "eCsID534RLMb"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bootcampDev",
   "language": "python",
   "name": "bootcampdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
