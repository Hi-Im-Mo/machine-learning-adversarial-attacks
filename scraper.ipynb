{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc572e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import splinter\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bf1ad",
   "metadata": {},
   "source": [
    "# Setting up our scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eded8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Browser session\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6732a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RESTART the df when compiling new filter_ data\n",
    "# df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ba3a4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an inidividual Walmart link for testing\n",
    "filter_ = 1\n",
    "# Create URL object\n",
    "website = f'https://www.walmart.com/reviews/product/389642865?filter={filter_}'\n",
    "# Visit website in our Browser session\n",
    "browser.visit(website)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8ad5ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important to pause briefly after the landing page loads because the bots detect rapid activity\n",
    "# and classify as spider\n",
    "\n",
    "# Scrape the landing page html\n",
    "html = browser.html\n",
    "# Use Beautiful soup to parse the webpage\n",
    "soup = BeautifulSoup(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "74db986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty reviews list\n",
    "review_list = []\n",
    "# Save all of the review containers into a list object\n",
    "rows = soup.find_all(\"li\", class_=\"dib w-100 mb3\")\n",
    "\n",
    "# Create a for loop to loop through the containers and pick up the data, text & rating\n",
    "for i in rows:\n",
    "    # Try for text data\n",
    "    try:\n",
    "        # Save date text\n",
    "        date = i.find('div', class_='f7 gray mt1').text\n",
    "        # Save review text\n",
    "        text = i.find('span', class_='tl-m mb3 db-m').text\n",
    "        # Save stars text\n",
    "        stars = i.find('span', class_='w_iUH7').text\n",
    "        # Add info dict to the empty reviews_list\n",
    "        review_list.append( {\n",
    "            'date':date,\n",
    "            'text':text,\n",
    "            'stars':stars\n",
    "            } )\n",
    "    # If text is empty, print \"empty text\" to terminal\n",
    "    except AttributeError:\n",
    "        print(\"empty text\")\n",
    "\n",
    "# Transform list of dicts into a df\n",
    "review_df = pd.DataFrame(review_list)\n",
    "# Append review_df to the master df\n",
    "df = pd.concat([df,review_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f19bcfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually kill session\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f87b01a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length check\n",
    "df = df.drop_duplicates()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5aa595",
   "metadata": {},
   "source": [
    "# Automating the scraping bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fbc9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RESTART the df when compiling new filter_ data\n",
    "# df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "880ead29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Length of df -- 20 --\n",
      "-- Length of df -- 40 --\n",
      "-- Length of df -- 60 --\n",
      "-- Length of df -- 80 --\n",
      "empty text\n",
      "-- Length of df -- 99 --\n",
      "empty text\n",
      "-- Length of df -- 118 --\n",
      "-- Length of df -- 138 --\n",
      "empty text\n",
      "-- Length of df -- 157 --\n",
      "-- Length of df -- 177 --\n",
      "-- Length of df -- 197 --\n",
      "-- Length of df -- 217 --\n",
      "-- Length of df -- 237 --\n",
      "-- Length of df -- 257 --\n",
      "-- Length of df -- 277 --\n",
      "-- Length of df -- 297 --\n",
      "-- Length of df -- 317 --\n",
      "-- Length of df -- 337 --\n",
      "-- Length of df -- 357 --\n",
      "-- Length of df -- 377 --\n",
      "-- Length of df -- 397 --\n",
      "-- Length of df -- 417 --\n",
      "-- Length of df -- 437 --\n",
      "-- Length of df -- 457 --\n",
      "-- Length of df -- 477 --\n",
      "-- Length of df -- 497 --\n",
      "-- Length of df -- 517 --\n",
      "-- Length of df -- 537 --\n",
      "-- Length of df -- 557 --\n",
      "-- Length of df -- 577 --\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of stars for the review\n",
    "# filter_ = 1\n",
    "# Create a variable that increases proportionally to the average increase in df size \n",
    "# to simulate the last page that the spider was on if it got interuptted or crashed\n",
    "last_page_num = int(round((len(df) / 20),0))\n",
    "# Initialize a full range of numbers through the 29\n",
    "page_num_list = list(range(30))\n",
    "# Trim off the 0 and 1 (not needed)\n",
    "page_num_list = page_num_list[(last_page_num+1):]\n",
    "\n",
    "# Create a for loop to loop through the containers and pick up the data, text & rating\n",
    "for i in page_num_list:\n",
    "\n",
    "    # Conditional to check if this is the first run or not\n",
    "    if int(len(df)) < 20:\n",
    "        # Initialize the Browser\n",
    "        browser = Browser('chrome')\n",
    "        # Compile URL with filter_\n",
    "        website = f'https://www.walmart.com/reviews/product/389642865?filter={filter_}'\n",
    "        # Sleep the terminal for 3 seconds to simulate human activity\n",
    "        sleep(3)\n",
    "        # Visit the website\n",
    "        browser.visit(website)        \n",
    "        # Sleep the terminal for 3 seconds to simulate human activity\n",
    "        sleep(3)\n",
    "        # Scrape the HTML of the landing page\n",
    "        html = browser.html\n",
    "        # Feed HTML to parser\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        \n",
    "        # Clear reviews list\n",
    "        review_list = []\n",
    "        # Save all of the review containers into a list object\n",
    "        rows = soup.find_all(\"li\", class_=\"dib w-100 mb3\")\n",
    "\n",
    "        # Create a for loop to loop through the containers and pick up the data, text & rating\n",
    "        for i in rows: \n",
    "            # Try for text data\n",
    "            try:\n",
    "                # Save date text\n",
    "                date = i.find('div', class_='f7 gray mt1').text\n",
    "                # Save review text\n",
    "                text = i.find('span', class_='tl-m mb3 db-m').text\n",
    "                # Save stars text\n",
    "                stars = i.find('span', class_='w_iUH7').text\n",
    "                # Add info dict to the empty reviews_list\n",
    "                review_list.append( {\n",
    "                    'date':date,\n",
    "                    'text':text,\n",
    "                    'stars':stars\n",
    "                } )\n",
    "            # If empty, print \"empty text\" to console\n",
    "            except AttributeError:\n",
    "                print(\"empty text\")\n",
    "        # Transform list of dicts into a df\n",
    "        review_df = pd.DataFrame(review_list)\n",
    "        # Append review_df to the master df\n",
    "        df = pd.concat([df, review_df])\n",
    "        # Print new length of master df to console\n",
    "        print(f\"-- Length of df -- {len(df)} --\")\n",
    "        # Kill the session\n",
    "        browser.quit()\n",
    "        # Sleep the terminal for random # between 110 and 140 seconds to simulate human activity\n",
    "        sleep(int(random.uniform(110,140)))\n",
    "    \n",
    "    # If this isn't the first run:\n",
    "    else:\n",
    "        # Initialize the Browser\n",
    "        browser = Browser('chrome')\n",
    "        # Compile URL with filter_\n",
    "        website = f'https://www.walmart.com/reviews/product/389642865?filter={filter_}&page={i}'\n",
    "        # Sleep the terminal for 3 seconds to simulate human activity\n",
    "        sleep(3)\n",
    "        # Visit the website\n",
    "        browser.visit(website)\n",
    "        # Sleep the terminal for 3 seconds to simulate human activity\n",
    "        sleep(3)\n",
    "        # Scrape the HTML of the landing page\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        review_list = []\n",
    "        # Save all of the review containers into a list object\n",
    "        rows = soup.find_all(\"li\", class_=\"dib w-100 mb3\")\n",
    "\n",
    "        # Create a for loop to loop through the containers and pick up the data, text & rating\n",
    "        for i in rows: \n",
    "            # Try for text data\n",
    "            try:\n",
    "                # Save date text\n",
    "                date = i.find('div', class_='f7 gray mt1').text\n",
    "                # Save review text\n",
    "                text = i.find('span', class_='tl-m mb3 db-m').text\n",
    "                # Save stars text\n",
    "                stars = i.find('span', class_='w_iUH7').text\n",
    "                # Add info dict to the empty reviews_list\n",
    "                review_list.append( {\n",
    "                    'date':date,\n",
    "                    'text':text,\n",
    "                    'stars':stars\n",
    "                } )\n",
    "            # If empty, print \"empty text\" to console\n",
    "            except AttributeError:\n",
    "                print(\"empty text\")\n",
    "        # Transform list of dicts into a df\n",
    "        review_df = pd.DataFrame(review_list)\n",
    "        # Append review_df to the master df\n",
    "        df = pd.concat([df, review_df])\n",
    "        # Print new length of master df to console\n",
    "        print(f\"-- Length of df -- {len(df)} --\")\n",
    "        # Kill the session\n",
    "        browser.quit()\n",
    "        # Sleep the terminal for random # between 110 and 140 seconds to simulate human activity\n",
    "        sleep(int(random.uniform(110,140)))\n",
    "        \n",
    "# Admittedly, there is a way to reduce the length of this cell and define the scraping pipeline as a function and \n",
    "# call that function whether its the first run or not, but this code was easier to visualize so for the time being\n",
    "# I am going to leave it the way it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "170bb4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/7/2024</td>\n",
       "      <td>Unfortunately the tv fell on me after purchasi...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/1/2023</td>\n",
       "      <td>I love Vizio that’s always my go to brand I ha...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/22/2023</td>\n",
       "      <td>I ordered a Vizio 50\" Class V-Series 4L UHD LE...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/5/2023</td>\n",
       "      <td>It didn't work right out of the box. Nothing b...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/24/2023</td>\n",
       "      <td>I ordered  75” Vizio TV and just put it up, bu...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11/28/2023</td>\n",
       "      <td>TV came damaged.  The box looks like it had be...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12/16/2023</td>\n",
       "      <td>Sound is terrible. literally have it on 100 an...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11/22/2023</td>\n",
       "      <td>Dislike I never received my order I was scamme...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11/24/2023</td>\n",
       "      <td>Like to get up date on when it's being delivered</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11/18/2023</td>\n",
       "      <td>Tv doesn’t connect to WiFi well and constantly...</td>\n",
       "      <td>1 out of 5 stars review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                               text  \\\n",
       "0     2/7/2024  Unfortunately the tv fell on me after purchasi...   \n",
       "1    11/1/2023  I love Vizio that’s always my go to brand I ha...   \n",
       "2   11/22/2023  I ordered a Vizio 50\" Class V-Series 4L UHD LE...   \n",
       "3    12/5/2023  It didn't work right out of the box. Nothing b...   \n",
       "4   12/24/2023  I ordered  75” Vizio TV and just put it up, bu...   \n",
       "..         ...                                                ...   \n",
       "15  11/28/2023  TV came damaged.  The box looks like it had be...   \n",
       "16  12/16/2023  Sound is terrible. literally have it on 100 an...   \n",
       "17  11/22/2023  Dislike I never received my order I was scamme...   \n",
       "18  11/24/2023   Like to get up date on when it's being delivered   \n",
       "19  11/18/2023  Tv doesn’t connect to WiFi well and constantly...   \n",
       "\n",
       "                      stars  \n",
       "0   1 out of 5 stars review  \n",
       "1   1 out of 5 stars review  \n",
       "2   1 out of 5 stars review  \n",
       "3   1 out of 5 stars review  \n",
       "4   1 out of 5 stars review  \n",
       "..                      ...  \n",
       "15  1 out of 5 stars review  \n",
       "16  1 out of 5 stars review  \n",
       "17  1 out of 5 stars review  \n",
       "18  1 out of 5 stars review  \n",
       "19  1 out of 5 stars review  \n",
       "\n",
       "[577 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length Check\n",
    "df = df.drop_duplicates()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f06d0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "# filter_ = 1\n",
    "df.to_csv(f'data/scraped_{filter_}star_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b70022e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill any remaining session\n",
    "# browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcampDev",
   "language": "python",
   "name": "bootcampdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
